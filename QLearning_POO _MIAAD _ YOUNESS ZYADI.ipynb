{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-OdC9LEa0wL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Class Environment**"
      ],
      "metadata": {
        "id": "viX1fd4SbNCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Environment:\n",
        "    def __init__(self, size, obstacles, start, goal ,strawberries):\n",
        "        self.size = size\n",
        "        self.grid = np.zeros(size)\n",
        "        self.strawberries_grid = np.zeros(size)\n",
        "        for x, y in obstacles:\n",
        "            self.grid[x][y] = 1\n",
        "        for x, y in strawberries:\n",
        "            self.strawberries_grid[x][y] = 15\n",
        "        self.start = start\n",
        "        self.goal = goal"
      ],
      "metadata": {
        "id": "P0UHLLxRbLDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Class QLearning**"
      ],
      "metadata": {
        "id": "_o0VJuZObRHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearning:\n",
        "    def __init__(self, environment, alpha, gamma, epsilon):\n",
        "        self.environment = environment\n",
        "        self.Q = np.zeros((environment.size[0], environment.size[1], 4))\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.actions = [\"H\", \"B\", \"G\", \"D\"]\n",
        "\n",
        "    def euclidean_distance(self, x1, y1, x2, y2):\n",
        "      if x1 == x2 and y1 == y2:\n",
        "        return 0\n",
        "      return np.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
        "\n",
        "    def learn(self, num_iterations):\n",
        "      for i in range(num_iterations):\n",
        "        # Sélection aléatoire de l'état de départ\n",
        "        x, y = self.environment.start\n",
        "\n",
        "        # Boucle de déplacement du robot\n",
        "        while (x, y) != self.environment.goal:\n",
        "          # Choix d'une action selon une stratégie d'exploration\n",
        "          if np.random.random() < self.epsilon:\n",
        "            action = np.random.choice(self.actions)\n",
        "          else:\n",
        "            action = self.actions[np.argmax(self.Q[x][y])]\n",
        "\n",
        "          # Effectuation de l'action\n",
        "          x_new, y_new = x, y\n",
        "          if action == \"H\":\n",
        "            x_new = x - 1\n",
        "          elif action == \"B\":\n",
        "            x_new = x + 1\n",
        "          elif action == \"G\":\n",
        "            y_new = y - 1\n",
        "          elif action == \"D\":\n",
        "            y_new = y + 1\n",
        "\n",
        "          # Vérifie si l'action n'amène pas le robot en dehors de l'environnement ou dans un obstacle\n",
        "          if (x_new < 0 or x_new >= self.environment.grid.shape[0] or y_new < 0 or y_new >= self.environment.grid.shape[1] or self.environment.grid[x_new][y_new] == 1):\n",
        "            reward = -10\n",
        "            x_new = x\n",
        "            y_new = y\n",
        "          else:\n",
        "            if reward < 0:\n",
        "              reward = 0\n",
        "            # Calcul de la récompense\n",
        "            elif (x_new, y_new) == self.environment.goal:\n",
        "              reward = 10\n",
        "            elif self.environment.strawberries_grid[x_new][y_new] > 0:\n",
        "              reward += self.environment.strawberries_grid[x_new][y_new]\n",
        "              self.environment.strawberries_grid[x_new][y_new] = 0\n",
        "            else:\n",
        "              reward = 1 / self.euclidean_distance(x_new, y_new, self.environment.goal[0], self.environment.goal[1])\n",
        "\n",
        "          # Mise à jour de la valeur Q\n",
        "          self.Q[x][y][self.actions.index(action)] = self.Q[x][y][self.actions.index(action)] + self.alpha * (reward + self.gamma * np.max(self.Q[x_new][y_new]) - self.Q[x][y][self.actions.index(action)])\n",
        "\n",
        "          # Mise à jour de la position du robot\n",
        "          x = x_new\n",
        "          y = y_new\n",
        "\n",
        "\n",
        "    def choose_action(self, x, y):\n",
        "      # Choose the action with the highest Q-value\n",
        "      return self.actions[np.argmax(self.Q[x][y])]\n",
        "\n",
        "    def choose_optimal_action(self, position):\n",
        "      x, y = position\n",
        "      optimal_action = self.actions[np.argmax(self.Q[x][y])]\n",
        "      return optimal_action\n",
        "\n",
        "    def visualize_q_values(self, position):\n",
        "      x, y = position\n",
        "      print(\"Q values at position ({}, {}):\".format(x, y))\n",
        "      for action in self.actions:\n",
        "        print(\"{}: {}\".format(action, self.Q[x][y][self.actions.index(action)]))\n",
        "\n",
        "    def visualize_q(self):\n",
        "      # Visualize the Q-values using a heatmap\n",
        "      plt.imshow(np.max(self.Q, axis=2))\n",
        "      plt.show()\n",
        "    def visualize_q_final(self):\n",
        "     # Print the final Q-values using a heatmap\n",
        "      for i in range(10):\n",
        "          for j in range(10):\n",
        "              max_q = np.max(self.Q[i][j])\n",
        "              action = self.actions[np.argmax(self.Q[i][j])]\n",
        "              print(\"{:.2f} ({})\".format(max_q, action), end=\" \")\n",
        "          print()\n",
        "    def print_best_path(self,environment):\n",
        "        x, y = environment.start\n",
        "        path = []\n",
        "        while (x, y) != environment.goal:\n",
        "            action = self.choose_action(x, y)\n",
        "            path.append((action, x, y))\n",
        "            if action == \"H\":\n",
        "                x -= 1\n",
        "            elif action == \"B\":\n",
        "                x += 1\n",
        "            elif action == \"G\":\n",
        "                y -= 1\n",
        "            elif action == \"D\":\n",
        "                y += 1\n",
        "        path.append((\"GOAL\", x, y)) \n",
        "        for i, (action, x, y) in enumerate(path):\n",
        "            if action == \"GOAL\":\n",
        "                print(f\"Step {i+1}: Goal reached at ({9}, {9})\")\n",
        "            else:\n",
        "                print(f\"Step {i+1}: Move {action} to ({x}, {y})\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DUX6uybzbYNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the environment with a size of 10x10, one obstacle at position (5, 5), a start position at (0, 0), and a goal position at (9, 9)\n",
        "environment = Environment((10, 10), [(0, 5), (0, 6), (3, 3), (3, 4), (6, 5),(6, 6)], (0, 0), (9, 9),[(1, 3), (4, 5), (7, 8)])\n",
        "\n",
        "# Create the Q-learning agent with a learning rate of 0.1, a discount factor of 0.9, and an exploration rate of 0.1\n",
        "agent = QLearning(environment, alpha=0.1, gamma=0.9, epsilon=0.3)\n",
        "\n",
        "#Train the agent for 1000 iterations\n",
        "agent.learn(10000)\n",
        "\n",
        "#Visualize the learned Q-values\n",
        "agent.visualize_q()\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "#Choose the optimal action at position (0, 0)\n",
        "optimal_action = agent.choose_optimal_action((0, 0))\n",
        "print(\"Optimal action at position (0, 0): {}\".format(optimal_action))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "#View the Q-values at position (0, 0)\n",
        "agent.visualize_q_values((0, 0))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Affichage de la valeur Q maximale et de l'action associée pour chaque état\n",
        "agent.visualize_q_final()\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Print the best path of the robot by choosing the action with the highest Q-value at each step\n",
        "agent.print_best_path(environment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rnzETPdCdxar",
        "outputId": "02092f89-0a8f-41a9-e3b0-cc2669c61244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Optimal action at position (0, 0): D\n",
            "\n",
            "\n",
            "Q values at position (0, 0):\n",
            "H: -7.31786806616606\n",
            "B: 2.957335322826736\n",
            "G: -7.318208617489612\n",
            "D: 2.9957227680505\n",
            "\n",
            "\n",
            "3.00 (D) 3.26 (B) 3.49 (B) 3.75 (D) 4.09 (B) 0.00 (H) 0.00 (H) 4.95 (B) 4.52 (B) 2.01 (G) \n",
            "3.25 (D) 3.52 (D) 3.80 (D) 4.10 (D) 4.42 (B) 4.81 (B) 5.04 (D) 5.47 (B) 5.50 (B) 4.27 (G) \n",
            "3.43 (D) 3.78 (D) 4.12 (D) 4.42 (D) 4.78 (D) 5.18 (B) 5.62 (B) 5.99 (B) 6.31 (B) 5.30 (G) \n",
            "3.17 (H) 3.44 (H) 3.74 (H) 0.00 (H) 0.00 (H) 5.59 (B) 6.07 (B) 6.58 (B) 6.84 (B) 6.86 (B) \n",
            "1.27 (H) 2.45 (H) 3.05 (H) 5.03 (D) 5.66 (D) 6.06 (B) 6.52 (D) 7.01 (B) 7.47 (B) 7.49 (B) \n",
            "0.05 (H) 0.01 (H) 0.96 (D) 4.22 (D) 5.15 (H) 6.51 (D) 6.94 (D) 7.48 (D) 8.04 (B) 7.98 (B) \n",
            "0.00 (H) 0.00 (H) 0.01 (H) 0.77 (H) 3.14 (B) 0.00 (H) 0.00 (H) 7.95 (D) 8.61 (B) 8.39 (B) \n",
            "0.00 (H) 0.00 (H) 0.01 (H) 0.42 (D) 5.80 (D) 7.39 (B) 8.11 (D) 8.54 (D) 9.16 (B) 8.79 (B) \n",
            "0.00 (H) 0.00 (H) 0.00 (H) 0.46 (D) 3.06 (H) 8.00 (D) 8.50 (D) 9.04 (D) 9.56 (B) 8.94 (G) \n",
            "0.00 (H) 0.00 (H) 0.00 (H) 0.06 (H) 1.50 (D) 6.63 (H) 8.52 (D) 9.11 (D) 9.76 (D) 0.00 (H) \n",
            "\n",
            "\n",
            "Step 1: Move D to (0, 0)\n",
            "Step 2: Move B to (0, 1)\n",
            "Step 3: Move D to (1, 1)\n",
            "Step 4: Move D to (1, 2)\n",
            "Step 5: Move D to (1, 3)\n",
            "Step 6: Move B to (1, 4)\n",
            "Step 7: Move D to (2, 4)\n",
            "Step 8: Move B to (2, 5)\n",
            "Step 9: Move B to (3, 5)\n",
            "Step 10: Move B to (4, 5)\n",
            "Step 11: Move D to (5, 5)\n",
            "Step 12: Move D to (5, 6)\n",
            "Step 13: Move D to (5, 7)\n",
            "Step 14: Move B to (5, 8)\n",
            "Step 15: Move B to (6, 8)\n",
            "Step 16: Move B to (7, 8)\n",
            "Step 17: Move B to (8, 8)\n",
            "Step 18: Move D to (9, 8)\n",
            "Step 19: Goal reached at (9, 9)\n"
          ]
        }
      ]
    }
  ]
}